{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lutorpy as lua\n",
    "require(\"nn\")\n",
    "require(\"optim\")\n",
    "require(\"cutorch\")\n",
    "require(\"cunn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_range = np.arange(80, 112000, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_indeces = np.load('src/time_train_indeces.npy')\n",
    "test_indeces = np.load('src/time_test_indeces.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M = M_star = 5\n",
    "N = NUMBER_OF_PEDESTRIANS\n",
    "X = Y = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "location = np.load('/pedestrians_location.npy') # [NUMBER_OF_PEDESTRIANS x NUMBER_OF_FRAMES x 2]\n",
    "location[:,:,0] = (X * location[:,:,0] / FRAME_WIDTH).astype(np.int64)\n",
    "location[:,:,1] = (Y * location[:,:,1] / FRAME_HEIGHT).astype(np.int64)\n",
    "# location[i,t] = [x,y] -- location of i-th pedestrian at time point t (aka frame 20*t) -- [0,0] for absent pedestians\n",
    "\n",
    "paths = np.load('src/all_not_ext_paths_with_ids.npy') # paths[j] = [i,t,x,y] -- all pedestrians' coordinates at all frames\n",
    "paths[:,2] = (X * paths[:,2] / FRAME_WIDTH).astype(np.int64)\n",
    "paths[:,3] = (Y * paths[:,3] / FRAME_HEIGHT).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# M-1 time points before curr_time and M_star after\n",
    "def get_t(curr_time, M, M_star):\n",
    "    step = 20\n",
    "    return np.arange(curr_time - step*(M-1), curr_time + step*(M_star+1), step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential()\n",
    "\n",
    "filters_num = 64\n",
    "kW = kH = 3\n",
    "kT = 1\n",
    "\n",
    "# https://github.com/torch/nn/blob/master/doc/convolution.md#nn.VolumetricConvolution\n",
    "model._add(nn.VolumetricConvolution(1, filters_num, 2*M, kW, kH, 1, 1, 1, 0)) # conv1\n",
    "model._add(nn.VolumetricConvolution(filters_num, filters_num, kT, kW, kH, 1, 1, 1, 0, 1, 1)) # conv2\n",
    "model._add(nn.VolumetricConvolution(filters_num, filters_num, kT, kW, kH, 1, 1, 1, 0, 1, 1)) # conv3\n",
    "\n",
    "# https://github.com/torch/nn/blob/master/doc/convolution.md#nn.VolumetricMaxPooling\n",
    "model._add(nn.VolumetricMaxPooling(1, 2, 2, 1, 2, 2))\n",
    "\n",
    "# https://github.com/torch/nn/blob/master/doc/simple.md#nn.CAdd\n",
    "model._add(nn.CAdd(1, int((Y-1)/2), int((X-1)/2))) # bias\n",
    "\n",
    "model._add(nn.VolumetricConvolution(filters_num, filters_num, kT, kW, kH, 1, 1, 1, 0, 1, 1)) # conv4\n",
    "model._add(nn.VolumetricConvolution(filters_num, filters_num, kT, kW, kH, 1, 1, 1, 0, 1, 1)) # conv5\n",
    "model._add(nn.VolumetricConvolution(filters_num, filters_num, kT, kW, kH, 1, 1, 1, 0, 1, 1)) # conv6\n",
    "\n",
    "# https://github.com/torch/nn/blob/master/doc/convolution.md#spatialfullconvolution\n",
    "model._add(nn.VolumetricFullConvolution(filters_num, 1, 2*M_star, 4, 4, 1, 2, 2)) # deconv\n",
    "\n",
    "criterion = nn.MSECriterion()\n",
    "\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def from_numpy(array):\n",
    "    return torch.fromNumpyArray(array)\n",
    "\n",
    "def to_numpy(array):\n",
    "    return (array.asNumpyArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "for curr_time in tqdm(time_range[train_indeces[:10]]):\n",
    "    t = get_t(curr_time, M, M_star)\n",
    "    ind_t = (t/20).astype(int) # for indexing in location\n",
    "\n",
    "    pedestrians_in_scene = paths[paths[:,1] == t[M-1]] # getting all pedestrians who located on map at curr_time\n",
    "\n",
    "    disp_volume = np.zeros((X, Y, 2*M))\n",
    "    disp_volume_star = np.zeros((X, Y, 2*M_star))\n",
    "\n",
    "    for p in pedestrians_in_scene:\n",
    "        l = location[p[0],ind_t[:M]] # locations of pedestrian at t_1, t_2, ..., t_M time points\n",
    "        l_star = location[p[0],ind_t[M:]]\n",
    "\n",
    "        # filling zeros absent pedestrians\n",
    "        for i, row in enumerate(l):\n",
    "            if (row == 0).all():\n",
    "                l[i] = l[-1] + np.array([X, Y]) # to get zero in displacement vector\n",
    "\n",
    "        for i, row in enumerate(l_star):\n",
    "            if (row == 0).all():\n",
    "                l_star[i] = l_star[-1] + np.array([X, Y])\n",
    "\n",
    "        d = (l[-1] - l) / np.array([X, Y]) + 1\n",
    "        d_star = (l_star[-1] - l_star) / np.array([X, Y]) + 1\n",
    "        disp_volume[p[2], p[3]] = d.ravel() # put in [X, Y]-th cell of disp_volume displacement vector\n",
    "        disp_volume_star[location[p[0],ind_t[-1]][0], location[p[0],ind_t[-1]][1]] = d_star.ravel()    \n",
    "\n",
    "    mask = (disp_volume_star != 0).astype(int)\n",
    "    \n",
    "    for _ in np.arange(2):\n",
    "        pred = to_numpy(model._forward(from_numpy(disp_volume.T[np.newaxis])))\n",
    "        criterion._forward(from_numpy(pred*(mask.T[np.newaxis])), from_numpy((disp_volume_star).T[np.newaxis]))\n",
    "        model._zeroGradParameters()\n",
    "        model._backward(from_numpy(disp_volume.T[np.newaxis]),\n",
    "                        criterion._backward(from_numpy(pred*(mask.T[np.newaxis])), from_numpy(disp_volume_star.T[np.newaxis])))\n",
    "        model._updateParameters(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# THERE IS A PROBLEM HERE:\n",
    "# we lose pedestrians' ids after coding into disp_volume so to decode we need somehow match previous path and predicted\n",
    "\n",
    "# displacement volume decode\n",
    "xs, ys = disp_volume[:,:,0].nonzero() # getting indices of non-zero predicted coordinates at first (t_M+1) timepoint\n",
    "for i, j in zip(xs,ys):\n",
    "    np.array([i,j]) - (disp_volume[i,j].reshape(-1,2) - 1) * np.array([X, Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
